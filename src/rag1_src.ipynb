{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import MeCab\n",
    "import uuid\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SEARCH_ENDPOINT = os.getenv(\"SEARCH_ENDPOINT\")\n",
    "SEARCH_KEY = os.getenv(\"SEARCH_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  \n",
    "OPENAI_MODEL_NAME_JUDGE = \"gpt-4o-mini\"\n",
    "OPENAI_MODEL_NAME_ANS = \"gpt-4o\"\n",
    "OPENAI_EMB_NAME = \"text-embedding-3-large\"\n",
    "SEARCH_INDEX_NAME=\"rag1_index\"\n",
    "\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "search_credential = AzureKeyCredential(SEARCH_KEY)\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY) \n",
    "index_client = SearchIndexClient(endpoint=SEARCH_ENDPOINT, credential=search_credential)\n",
    "jinja_env = Environment(loader=FileSystemLoader('prompts'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.インデックス作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードでインデックスを２種類作成する。\n",
    "- chunk_size:2056, overlap:10のインデックス : rag-1_index\n",
    "- chunk_size:512, overlap:0のインデックス : rag-1_index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index():\n",
    "\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchableField(name=\"author\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"content_trans\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=3072, vector_search_profile_name=\"vector-profile\"),\n",
    "        SearchableField(name=\"keywords\", type=SearchFieldDataType.String),\n",
    "    ]\n",
    "\n",
    "    vector_search = create_vector_search()\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=SEARCH_INDEX_NAME,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = index_client.create_or_update_index(index)\n",
    "        print(f\"Index created or updated: {result.name}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating index: {str(e)}\")\n",
    "        print(f\"Index definition: {index.serialize()}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_vector_search(\n",
    "        algorithm_name=\"my-algorithms-config\",\n",
    "        vector_search_profile_name=\"vector-profile\"\n",
    "        ):\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=vector_search_profile_name,\n",
    "                algorithm_configuration_name=algorithm_name,\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=algorithm_name)],\n",
    "    )\n",
    "    return vector_search\n",
    "\n",
    "def chunk_text(text, max_chunk_size=512, overlap_sentences=0):\n",
    "    lines = text.split('\\n')\n",
    "    title = lines[0].strip()\n",
    "    author = lines[1].strip()\n",
    "    content = '\\n'.join(lines[2:])\n",
    "    sentences = content.replace('。', '。\\n').split('\\n')\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    overlap_buffer = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            overlap_buffer = overlap_buffer[-overlap_sentences:]\n",
    "            current_chunk = ''.join(overlap_buffer)\n",
    "        \n",
    "        current_chunk += sentence\n",
    "        overlap_buffer.append(sentence)\n",
    "\n",
    "        if len(overlap_buffer) > overlap_sentences:\n",
    "            overlap_buffer.pop(0)\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return title, author, chunks\n",
    "\n",
    "\n",
    "def translate_chunks(chunks):\n",
    "    template = jinja_env.get_template('translate.jinja')\n",
    "    prompt = template.render(chunk=chunks)\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL_NAME_JUDGE,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": \"指示を実行してください。\"}\n",
    "        ]\n",
    "    )\n",
    "    translated_result = response.choices[0].message.content\n",
    "    \n",
    "    return translated_result\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tagger = MeCab.Tagger()\n",
    "\n",
    "    text = re.sub(r'《.*?》', '', text)\n",
    "    text = re.sub(r'［＃.*?］', '', text)\n",
    "    text = re.sub(r'｜', '', text)\n",
    "\n",
    "    node = tagger.parseToNode(text)\n",
    "\n",
    "    words = []\n",
    "    while node:\n",
    "        features = node.feature.split(',')\n",
    "        if features[0] == '名詞' :\n",
    "            words.append(node.surface)\n",
    "        node = node.next\n",
    "\n",
    "    # 重複を除去し、出現順序を保持\n",
    "    unique_keywords = []\n",
    "    seen = set()\n",
    "    for keyword in words:\n",
    "        if keyword not in seen:\n",
    "            unique_keywords.append(keyword)\n",
    "            seen.add(keyword)\n",
    "\n",
    "    return ', '.join(unique_keywords)\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-large\" \n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def process_and_register_document(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    title, author, chunks = chunk_text(text)\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        keywords = extract_keywords(chunk)\n",
    "        translated_result = translate_chunks(chunk)\n",
    "        result_vec =\" \".join([chunk, translated_result,keywords])\n",
    "        vector = get_embedding(result_vec)\n",
    "\n",
    "        document = {\n",
    "            \"id\": f\"{str(uuid.uuid4())}_chunk_{i+1}\",\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"content\": chunk,\n",
    "            \"content_trans\": translated_result,\n",
    "            \"content_vector\": vector,\n",
    "            \"keywords\": keywords,\n",
    "        }\n",
    "        documents.append(document)\n",
    "        \n",
    "        print(str(uuid.uuid4())+f\"_chunk_{i+1}:\")\n",
    "        print(f\"  Title: {title}\")\n",
    "        print(f\"  Author: {author}\")\n",
    "        print(f\"  Content: {chunk[:50]}...\")\n",
    "        print(f\"  Content_Translated: {translated_result[:50]}...\")\n",
    "        print(f\"  Vector (first 5 elements): {vector[:5]}\")\n",
    "        print(f\"  Keywords: {keywords[:10]}...\")\n",
    "        print(\"--\" * 10)\n",
    "\n",
    "    result = search_client.upload_documents(documents)\n",
    "    print(f\"Uploaded {len(result)} documents from {file_path}\")\n",
    "    return len(documents)\n",
    "\n",
    "def main():\n",
    "    create_search_index()\n",
    "    data_folder = \"./data\"\n",
    "    total_chunks = 0\n",
    "    \n",
    "    for file_path in glob.glob(os.path.join(data_folder, \"*.txt\")):\n",
    "        num_chunks = process_and_register_document(file_path)\n",
    "        total_chunks += num_chunks\n",
    "    \n",
    "    print(f\"Total number of chunks registered: {total_chunks}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RAG回答作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_to_token_limit(text: str, limit: int) -> str:\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) <= limit:\n",
    "        return text\n",
    "    return encoding.decode(tokens[:limit])\n",
    "\n",
    "def read_query_csv(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return list(reader)\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=OPENAI_EMB_NAME\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def hybrid_search(problem, query, filter_title, top_k, query_type):\n",
    "    if query_type in [\"計算型\", \"グローバル文脈解釈型\"]:\n",
    "        search_client = SearchClient(endpoint=SEARCH_ENDPOINT, index_name=\"rag-1_index2\", credential=search_credential)\n",
    "    elif query_type in [\"選択肢型\", \"ローカル文脈解釈型\", \"人物・モノ列挙型\"]:\n",
    "        search_client = SearchClient(endpoint=SEARCH_ENDPOINT, index_name=\"rag-1_index\", credential=search_credential)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid query_type provided.\")\n",
    "\n",
    "    combined_query = f\"{query}{problem} \"\n",
    "    query_vector = get_embedding(combined_query)\n",
    "    \n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=top_k,\n",
    "        fields=\"content_vector\"\n",
    "    )\n",
    "\n",
    "    keyword_query = f\"{query}^3{problem} \"\n",
    "\n",
    "    filter_expression = f\"search.in(title, '{filter_title}')\" if filter_title != \"不明\" else None\n",
    "\n",
    "    results = search_client.search(\n",
    "        search_text=keyword_query,\n",
    "        vector_queries=[vector_query],\n",
    "        filter=filter_expression,\n",
    "        select=[\"id\",\"title\", \"author\", \"content\", \"content_trans\", \"keywords\"],\n",
    "        top=top_k,\n",
    "    )\n",
    "\n",
    "    search_results = list(results)\n",
    "\n",
    "    filtered_results = [\n",
    "        {\n",
    "            \"content\": result[\"content\"],\n",
    "        } for result in search_results\n",
    "    ]\n",
    "\n",
    "    for i, content in enumerate(filtered_results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"Content: {content}\")  \n",
    "        print(\"\\n\") \n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "def query_intent(query,title):\n",
    "    template = jinja_env.get_template(\"query_intent.jinja\")\n",
    "\n",
    "    system_message =template.render(query=query,title=title)  \n",
    "\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"provide_intent\",\n",
    "            \"description\": \"質問の意図を抽出する\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"intent\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"質問意図\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"intent\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL_NAME_ANS,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": \"指示を実行する\"}\n",
    "        ],\n",
    "        functions=functions,\n",
    "        function_call={\"name\": \"provide_intent\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    function_call = response.choices[0].message.function_call\n",
    "    result = json.loads(function_call.arguments)\n",
    "    \n",
    "    return result[\"intent\"]\n",
    "\n",
    "def generate_answer_draft(context, query, title, cls):\n",
    "    # clsに基づいて適切なテンプレートを選択\n",
    "    template_map = {\n",
    "        \"キーワード探索型\": \"answer_process_q1.jinja\",\n",
    "        \"グローバル文脈解釈型\": \"answer_process_q2.jinja\",\n",
    "        \"ローカル文脈解釈型\": \"answer_process_q2.jinja\",\n",
    "        \"計算型\": \"answer_process_q3.jinja\",\n",
    "        \"選択肢型\": \"answer_process_q4.jinja\",\n",
    "        \"人物・モノ列挙型\": \"answer_process_q5.jinja\"\n",
    "    }\n",
    "    \n",
    "    template_name = template_map.get(cls)\n",
    "    template = jinja_env.get_template(template_name)\n",
    "\n",
    "    context = json.dumps(context, ensure_ascii=False)\n",
    "\n",
    "    if template_name == \"answer_process_q1.jinja\":\n",
    "        system_message = template.render(\n",
    "            title=title,\n",
    "            query=query\n",
    "        )\n",
    "    else:\n",
    "        system_message = template.render(\n",
    "            context=context,\n",
    "            query=query\n",
    "        )\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL_NAME_ANS,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": \"指示を実行してください。\"}\n",
    "         ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    answer_draft = response.choices[0].message.content\n",
    " \n",
    "    print(answer_draft)\n",
    "    \n",
    "    return answer_draft\n",
    "\n",
    "def generate_answer(answer_draft):\n",
    "    template = jinja_env.get_template(\"answer_generate.jinja\")\n",
    "\n",
    "    system_message =\"あなたは小説に関する質問に対して正確な回答をするAIアシスタントです。\"\n",
    "    user_message =template.render(answer_draft=answer_draft)\n",
    "\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"provide_answer_and_evidence\",\n",
    "            \"description\": \"回答と根拠を提供する\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"answer\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"回答(50トークン以内)\"\n",
    "                    },\n",
    "                    \"evidence\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"根拠（50トークン以内）\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"answer\", \"evidence\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL_NAME_ANS,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        functions=functions,\n",
    "        function_call={\"name\": \"provide_answer_and_evidence\"},\n",
    "        temperature=1\n",
    "    )\n",
    "    \n",
    "    function_call = response.choices[0].message.function_call\n",
    "    result = json.loads(function_call.arguments)\n",
    "    result[\"answer\"] = truncate_to_token_limit(result[\"answer\"], 50)\n",
    "    result[\"evidence\"] = truncate_to_token_limit(result[\"evidence\"], 50)\n",
    "    \n",
    "    print(result)\n",
    "    print(\"---\" * 30)\n",
    "    \n",
    "    return result[\"answer\"], result[\"evidence\"]\n",
    "\n",
    "def _count_keyword_in_file(file_path, keyword_pattern):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        if keyword_pattern.startswith(\"r'\") and keyword_pattern.endswith(\"'\"):\n",
    "            keyword_pattern = keyword_pattern[2:-1]\n",
    "\n",
    "        count = len(re.findall(keyword_pattern, text))\n",
    "        result = {\n",
    "            \"answer\": f\"{count} 回\",\n",
    "            \"evidence\": \"文章検索の結果より\"\n",
    "        }\n",
    "        return result\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ファイル {file_path} が見つかりませんでした。\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        return None\n",
    "\n",
    "def search_query_answer(query):\n",
    "    system_message=\"\"\"\"\n",
    "    #役割\n",
    "    あなたは質問の中の検索キーワードを抜き出すプロフェッショナルです。\n",
    "    \n",
    "    #指示\n",
    "    - 質問を解釈し、検索すべきキーワードを抜き出してください。\n",
    "    - 文字列の出現回数はとはないといった場合は正規表現としてください。\n",
    "    \n",
    "    #例\n",
    "    質問 : 「田中」という名前は何回登場しますか？\n",
    "    抽出キーワード：田中\n",
    "    \n",
    "    質問: 「おお…」（おの回数は問わない）という声が登場する回数は何回ですか？\n",
    "    抽出キーワード : r'(おお)+'\n",
    "    \n",
    "    #Tools\n",
    "    keywordの変数に抽出キーワードを格納する\n",
    "    \"\"\"\n",
    "\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"provide_keyword\",\n",
    "            \"description\": \"質問内の検索キーワードを抽出する\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"検索の抽出キーワード\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"keyword\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL_NAME_ANS,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"#質問 : {query}\"}\n",
    "        ],\n",
    "        functions=functions,\n",
    "        function_call={\"name\": \"provide_keyword\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    function_call = response.choices[0].message.function_call\n",
    "    keyword = json.loads(function_call.arguments)['keyword']\n",
    "    print(keyword)\n",
    "    \n",
    "    result = _count_keyword_in_file(\"./data/combined/all.txt\", keyword)\n",
    "\n",
    "    if result is None:\n",
    "        return \"検索結果が見つかりませんでした。\", \"エラーが発生したか、ファイルが見つかりませんでした。\"\n",
    "    else:\n",
    "        print(f'回答:{result[\"answer\"]}  根拠:{result[\"evidence\"]}')\n",
    "        print(\"---\" * 30)\n",
    "        return result[\"answer\"], result[\"evidence\"]\n",
    "\n",
    "def judge_title(query):\n",
    "    template = jinja_env.get_template('judge_title.jinja')\n",
    "    system_message = template.render()\n",
    "    user_message = f\"質問: {query}\"\n",
    "    \n",
    "    functions = [{\n",
    "        \"name\": \"provide_title\",\n",
    "        \"description\": \"小説タイトルを提供する\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"string\", \"description\": '判断された小説タイトル [\"不如帰\",\"流行暗殺節\",\"カインの末裔\",\"競漕\",\"芽生\",\"サーカスの怪人\",\"死生に関するいくつかの断想\",\"不明\"]'}\n",
    "            },\n",
    "            \"required\": [\"title\"]\n",
    "        }\n",
    "    }]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL_NAME_ANS,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        functions=functions,\n",
    "        function_call={\"name\": \"provide_title\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    return result[\"title\"]\n",
    "\n",
    "def judge_query(query):\n",
    "    template = jinja_env.get_template('judge_query.jinja')\n",
    "    system_message = template.render()\n",
    "    user_message = f\"質問: {query}\"\n",
    "    \n",
    "    functions = [{\n",
    "        \"name\": \"judge_query\",\n",
    "        \"description\": \"質問の分類型を提供する\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"cls\": {\"type\": \"string\", \"description\": '判断された質問の種類 [\"選択肢型\",\" 計算型\",\"グローバル文脈解釈型\",\"ローカル文脈解釈型\",\"キーワード探索型\",\"キーワード特定型\",\"人物・モノ列挙型\"]'}\n",
    "            },\n",
    "            \"required\": [\"cls\"]\n",
    "        }\n",
    "    }]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL_NAME_ANS,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        functions=functions,\n",
    "        function_call={\"name\": \"judge_query\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    return result[\"cls\"]\n",
    "\n",
    "def process_queries(queries, trial_num):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        index = query['index']\n",
    "        problem = query['problem']\n",
    "        title = judge_title(problem)\n",
    "        cls = judge_query(problem)\n",
    "        \n",
    "\n",
    "        problem_intent=query_intent(query=problem,title=title)\n",
    "\n",
    "        print(problem_intent)\n",
    "        if cls==\"キーワード特定型\":\n",
    "            print(f\"TYPE1:{cls}\")\n",
    "            answer,evidence=search_query_answer(query)\n",
    "\n",
    "        elif cls==\"キーワード探索型\":\n",
    "            print(f\"TYPE2:{cls}\")\n",
    "            answer_draft = generate_answer_draft(context=\"\", query=problem, title=title,cls=cls)\n",
    "            answer,evidence= generate_answer(answer_draft)\n",
    "\n",
    "        elif cls in [\"計算型\", \"グローバル文脈解釈型\"]:\n",
    "            print(f\"TYPE3:{cls}\")\n",
    "            context = hybrid_search(problem=problem, query=problem_intent, filter_title=title, top_k=10, query_type=cls)\n",
    "            answer_draft = generate_answer_draft(context=context, query=problem, title=title,cls=cls)\n",
    "            answer,evidence = generate_answer(answer_draft)\n",
    "\n",
    "        else:\n",
    "            print(f\"TYPE4:{cls}\")\n",
    "            context = hybrid_search(problem=problem, query=problem_intent, filter_title=title, top_k=4, query_type=cls)\n",
    "            answer_draft = generate_answer_draft(context=context, query=problem, title=title,cls=cls)\n",
    "            answer,evidence = generate_answer(answer_draft)\n",
    "\n",
    "\n",
    "        results.append([index, answer, evidence])\n",
    "        \n",
    "    output_file = f\"../submit/submit_{trial_num}.csv\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(results)\n",
    "\n",
    "def main():\n",
    "    trial_num = \"001\"\n",
    "    query_file = \"../input/query.csv\"\n",
    "    \n",
    "    queries = read_query_csv(query_file)\n",
    "    process_queries(queries, trial_num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. submit用zipファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def create_submission_zip_from_csv(input_csv, output_zip=\"submission.zip\"):\n",
    "    # Create a temporary directory for the submission structure\n",
    "    temp_dir = \"temp_submit\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Copy the input CSV file to the temporary directory and rename it to predictions.csv\n",
    "    shutil.copy(input_csv, os.path.join(temp_dir, \"predictions.csv\"))\n",
    "\n",
    "    # Create a ZIP file\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, temp_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "    # Remove the temporary directory\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    print(f\"Submission ZIP file created: {output_zip}\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = \"../submit/submit_001.csv\"\n",
    "    output_zip_file = \"../submit/submission_001.zip\"\n",
    "    create_submission_zip_from_csv(input_csv_file, output_zip_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
